











import torch 
from torch import nn
import numpy as np
import matplotlib.pyplot as plt 








# create known parameters 
weight = 0.8
bias = 0.3

X = torch.arange(start=0, end=1, step=0.02).unsqueeze(dim=1)
y = weight*X + bias
X[:10,], y[:10]


len(X), len(y)





train_split = int(0.8*len(X))

X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)





def plot_predictions(train_data = X_train,
                     train_label = y_train,
                     test_data = X_test,
                     test_label = y_test,
                     predictions = None):
    """
    plots the training, test data and compare predictions.
    """
    plt.figure(figsize=(9,6))
  
    # Plot training data in blue
    plt.scatter(train_data, train_label, color='b', s=4, label='Training Data')
    
    # Plot the test data in green
    plt.scatter(test_data, test_label, color='g', s=4, label='Test Data')

    # Plot predictions if they exist
    if predictions is not None:
        plt.scatter(test_data, predictions, color='r', s=4, label='Predictions')

    # Show the legend
    plt.legend(prop={'size': 14})


plot_predictions()











# Create Linear Regression model class 
class LinearRegressionModel(nn.Module):  # almost everything in PyTorch inherits from nn.Module
    def __init__(self):
        super().__init__()

        # Initialize model parameters
        self.weights = nn.Parameter(torch.randn(1,    # start with a random weight and  
                                                requires_grad=True,  # can this parameter be updated via gradient descent
                                                dtype=torch.float))  # PyTorch loves torch.float32 dtype
        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))

    # Forward method to define the computation in the method
    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x is the input data 
        return self.weights*x + self.bias  # Linear Regression formula














# Create a random seed 
torch.manual_seed(42)

# create a subclass of the model (this is the instance of nn.Module)
model_0 = LinearRegressionModel()

#check out the paremeters
list(model_0.parameters())


# List named parameters
model_0.state_dict()





# Make predictions with model
with torch.inference_mode():  # doesn't keep gradients and doesn't store computation graph
    y_preds = model_0(X_test)

# Note: in older PyTorch code you might also see torch.no_grad()
# with torch.no_grad():
#   y_preds = model_0(X_test)
# torch.no_grad() stores computation graph


plot_predictions(predictions=y_preds)








# List model parameters 
list(model_0.parameters())


model_0.state_dict()


# Setup the loss function
loss_fn = nn.L1Loss()

# Setup the optimizer
optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)

















from tqdm import tqdm

torch.manual_seed(42)

# create the no. of loop 
epochs = 300

# Track the different values
epoch_count = []
train_loss_values = []
test_loss_values = []

# 0. loop through the data
for epoch in tqdm(range(epochs)):
    # Set the model to train mmodel
    model_0.train()
    
    # 1. Forward Propagation
    y_pred = model_0(X_train)

    # 2. Calculate the loss 
    loss = loss_fn(y_pred, y_train)

    # 3. Optimizer zero grad
    optimizer.zero_grad()

    # 4. Perform Back Propagation wrt the parameters
    loss.backward()

    # 5. Step the optimizer (Gradient Descent)
    optimizer.step()  # by default how the optimizer changes will accumulate through the loop
                      # so we have to zero them in step 3 for the next iteration of loop 
    ## Testing
    model_0.eval()    # turns off different settings in the model not needed for evaluation/testing (dropout, batch-norm layers)
    with torch.inference_mode(): # turns off gradient tracking and doesn't store computation graph
        # 1. Forward Propagation
        test_pred = model_0(X_test)

        # 2. Claculate the loss 
        test_loss = loss_fn(test_pred, y_test)

        # print out what's happening every 10 epochs
        if epoch%10 == 0:
            epoch_count.append(epoch)
            train_loss_values.append(loss)
            test_loss_values.append(test_loss)
            print(f'Epoch: {epoch:3d}| Loss: {loss:.6f}| Test Loss: {test_loss:.6f}')

print(model_0.state_dict())
    
    
    

    


train_loss_values_np = np.array(torch.tensor(train_loss_values).numpy())
test_loss_values_np = np.array(torch.tensor(test_loss_values).numpy())


# Plot the loss curves
plt.figure(figsize=(6,4))
plt.plot(epoch_count, train_loss_values_np, label = 'Train Loss')
plt.plot(epoch_count, test_loss_values_np, label = 'Test Loss')
plt.title('Training loss vs Testing loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()





# 1. Set the model in evaluation mode
model_0.eval()

# 2. Setup the inference mode context manager
with torch.inference_mode():
  # 3. Make sure the calculations are done with the model and data on the same device
  # in our case, we haven't setup device-agnostic code yet so our data and model are
  # on the CPU by default.
  # model_0.to(device)
  # X_test = X_test.to(device)
  y_preds = model_0(X_test)
y_preds


plot_predictions(predictions=y_preds)








# Saving PyTorch Model
from pathlib import Path

# 1. Create models directory
MODEL_PATH = Path('models')
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create models save path 
MODEL_NAME = '01-PyTorch_Workflow_model_0.pth'
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Model Save
print(f'Model Saved to: {MODEL_SAVE_PATH}')
torch.save(obj = model_0.state_dict(), f=MODEL_SAVE_PATH)






# Create an instance of the model
loaded_model_0 = LinearRegressionModel()

# Load the state dict 
loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) # We have to deserialize the model state dict before loading in the new model


loaded_model_0.state_dict()





loaded_model_0.eval()
with torch.inference_mode():
    loaded_model_preds = loaded_model_0(X_test)
    loaded_loss = loss_fn(loaded_model_preds, y_test)

loaded_loss


plot_predictions(predictions=loaded_model_preds)



