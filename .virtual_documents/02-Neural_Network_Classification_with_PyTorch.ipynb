

















import sklearn


from sklearn.datasets import make_circles

# Make 100 samples
n_samples = 1000
X, y = make_circles(n_samples=n_samples,
                   noise=0.03,
                   random_state=42)

len(X), len(y)


# Print first five samples 
print(f"First five samples of X: \n{X[:5]}")
print(f"First five samples of y: \n{y[:5]}")


# Make dataframe
import pandas as pd 

circles = pd.DataFrame({"X1": X[:,0], "X2": X[:,1], "label":y})
circles.head(10)


 # Visualize
import matplotlib.pyplot as plt 
plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap=plt.cm.RdYlBu)





X.shape, y.shape


# View the first samples of feature and label
print(f"Values for one sample of X: {X[0]} and for y: {y[0]}")
print(f"Shapes for one sample of X: {X[0].shape} and y: {y[0].shape}")





# View the current type of X and y
type(X), X.dtype, y.dtype


import torch

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

type(X), X.dtype, y.dtype


torch.manual_seed(42)


# Create train, test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.2, 
                                                   random_state=42)

len(X_train), len(X_test), len(y_train), len(y_test)








# Import pytorch and nn 
import torch
from torch import nn

# 1. Setup device agnostic code 
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device





# 1. Construct a model that subclasses nn.Module 
class CircleModelV0(nn.Module):
    def __init__(self):
        super().__init__()
        # 2. Create 2 nn.Linear layers capable of handling shapes of out data 
        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 featuers and upscales to 5 features
        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layear and outputs a single feature (same as y)

    # 3. Define a forward() method that outlines the forward pclass 
    def forward(self, x): 
        return self.layer_2(self.layer_1(x))  # x -> layer_1 -> layer_2 -> output

# Create an instance of our model class and move it to current devie 
model_0 = CircleModelV0().to(device)

model_0
        
        


# Let's replicate the model using nn.Sequential
model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0


# Make predictons
with torch.inference_mode():
    untrained_preds = model_0(X_test.to(device))
untrained_preds[:10]








# Setup loss function 
loss_fn = torch.nn.BCEWithLogitsLoss() # sigmoid activation function buit-in

# Setup optimizer
optimizer = torch.optim.SGD(params=model_0.parameters(), 
                           lr=0.1)


# Calculate accuracy
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct/len(y_pred)) * 100
    return acc








# View the frist 5 outputs of the forward pass on the test data
model_0.eval()
with torch.inference_mode():
    y_logits = model_0(X_test.to(device))[:5]
y_logits





# Use sigmoid on model logits
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs





# Find the predicted labels
y_preds = torch.round(y_pred_probs)

# In full. (logits -> pred probabilities -> pred labels)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

# checking for equality
print(torch.eq(y_pred_labels.squeeze(), y_preds.squeeze()))

y_preds.squeeze()





from tqdm.auto import tqdm
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# set the no. of epochs
epochs = 100

# Put the data the device 
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Build training and testing loop 
for epoch in tqdm(range(epochs)):
    ### Training
    model_0.train()
    
    # 1. Forward Pass 
    y_logits = model_0(X_train).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits))  # logits -> pred probs -> pred labels

    # 2. Calculate loss 
    loss = loss_fn(y_logits, y_train)  # torch.BCEWithLogitsLoss() has built-in sigmoid
    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)

    # 3. Optimizer zero grad 
    optimizer.zero_grad()

    # 4. Backprop (loss backward)
    loss.backward()

    # 5. step optimizer (gradient descent)
    optimizer.step()

    ### Testing 
    model_0.eval()

    # 1. Forward pass
    with torch.inference_mode():
        y_logits_test = model_0(X_test).squeeze()
        y_preds_test = torch.round(torch.sigmoid(y_logits_test))

    # 2. Claculate loss 
    test_loss = loss_fn(y_logits_test, y_test)
    test_acc = accuracy_fn(y_true=y_test, y_pred=y_preds_test)
    
    # Print out what's happening
    if epoch % 10 == 0:
        print(f"Epoch: {epoch:3d} | Loss: {loss:.5f} | Acc: {acc:.2f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}")








import requests
from pathlib import Path 

# Download helper functions from Learn PyTorch repo (if not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)

from helper_functions import plot_predictions, plot_decision_boundary


# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test)





class CircleModleV1(nn.Module): 
    def __init__(self): 
        super().__init__()
        self.layer_1 = nn.Linear(in_features=2, out_features=10)
        self.layer_2 = nn.Linear(in_features=10, out_features=10)
        self.layer_3 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x): 
        return self.layer_3(self.layer_2(self.layer_1(x))) 

model_1 = CircleModleV1().to(device)
model_1


# Setup loss function
loss_fn = torch.nn.BCEWithLogitsLoss() 

# Setup optimizer 
optimizer = torch.optim.SGD(params=model_1.parameters(), lr = 0.001)





torch.manual_seed(42)

epochs = 1000  # train longer that before

# Put data to target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device) 

for epoch in tqdm(range(epochs)): 
    ### Training 
    model_1.train()

    # 1. Forward pass 
    y_logits = model_1(X_train).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits))  # logits -> pred probs -> pred labels

    # 2 Calculate loss and accuracy 
    loss = loss_fn(y_logits, y_train)
    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)

    # 3. optimizer zero grad 
    optimizer.zero_grad() 

    # 4. Back Prop (loss backward) 
    loss.backward() 

    # 5. Step optimizer (gradient descent) 
    optimizer.step() 

    ### Testing 
    model_1.eval()
    with torch.inference_mode():
        # 1. Forward pass 
        y_logits_test = model_1(X_test).squeeze() 
        y_test = torch.sigmoid(y_logits_test) 
    
        # 2. Loss and accuracy 
        test_loss = loss_fn(y_logits_test, y_test)
        test_acc = accuracy_fn(y_true=y_test, y_pred=y_test) 

    # print what's happening 
    if epoch%100 == 0: 
        print(f"Epoch: {epoch:4d} | Loss: {loss:.5f} | Acc = {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")

    
        



