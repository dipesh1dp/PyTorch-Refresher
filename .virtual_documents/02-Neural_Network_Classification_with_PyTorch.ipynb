

















import sklearn


from sklearn.datasets import make_circles

# Make 100 samples
n_samples = 1000
X, y = make_circles(n_samples=n_samples,
                   noise=0.03,
                   random_state=42)

len(X), len(y)


# Print first five samples 
print(f"First five samples of X: \n{X[:5]}")
print(f"First five samples of y: \n{y[:5]}")


# Make dataframe
import pandas as pd 

circles = pd.DataFrame({"X1": X[:,0], "X2": X[:,1], "label":y})
circles.head(10)


 # Visualize
import matplotlib.pyplot as plt 
plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap=plt.cm.RdYlBu)





X.shape, y.shape


# View the first samples of feature and label
print(f"Values for one sample of X: {X[0]} and for y: {y[0]}")
print(f"Shapes for one sample of X: {X[0].shape} and y: {y[0].shape}")





# View the current type of X and y
type(X), X.dtype, y.dtype


import torch

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

type(X), X.dtype, y.dtype


torch.manual_seed(42)


# Create train, test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.2, 
                                                   random_state=42)

len(X_train), len(X_test), len(y_train), len(y_test)








# Import pytorch and nn 
import torch
from torch import nn

# 1. Setup device agnostic code 
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device





# 1. Construct a model that subclasses nn.Module 
class CircleModelV0(nn.Module):
    def __init__(self):
        super().__init__()
        # 2. Create 2 nn.Linear layers capable of handling shapes of out data 
        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 featuers and upscales to 5 features
        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layear and outputs a single feature (same as y)

    # 3. Define a forward() method that outlines the forward pclass 
    def forward(self, x): 
        return self.layer_2(self.layer_1(x))  # x -> layer_1 -> layer_2 -> output

# Create an instance of our model class and move it to current devie 
model_0 = CircleModelV0().to(device)

model_0
        
        


# Let's replicate the model using nn.Sequential
model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0


# Make predictons
with torch.inference_mode():
    untrained_preds = model_0(X_test.to(device))
untrained_preds[:10]








# Setup loss function 
loss_fn = torch.nn.BCEWithLogitsLoss() # sigmoid activation function buit-in

# Setup optimizer
optimizer = torch.optim.SGD(params=model_0.parameters(), 
                           lr=0.1)


# Calculate accuracy
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct/len(y_pred)) * 100
    return acc








# View the frist 5 outputs of the forward pass on the test data
model_0.eval()
with torch.inference_mode():
    y_logits = model_0(X_test.to(device))[:5]
y_logits





# Use sigmoid on model logits
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs





# Find the predicted labels
y_preds = torch.round(y_pred_probs)

# In full. (logits -> pred probabilities -> pred labels)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

# checking for equality
print(torch.eq(y_pred_labels.squeeze(), y_preds.squeeze()))

y_preds.squeeze()





from tqdm.auto import tqdm
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# set the no. of epochs
epochs = 100

# Put the data the device 
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Build training and testing loop 
for epoch in tqdm(range(epochs)):
    ### Training
    model_0.train()
    
    # 1. Forward Pass 
    y_logits = model_0(X_train).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits))  # logits -> pred probs -> pred labels

    # 2. Calculate loss 
    loss = loss_fn(y_logits, y_train)  # torch.BCEWithLogitsLoss() has built-in sigmoid
    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)

    # 3. Optimizer zero grad 
    optimizer.zero_grad()

    # 4. Backprop (loss backward)
    loss.backward()

    # 5. step optimizer (gradient descent)
    optimizer.step()

    ### Testing 
    model_0.eval()

    # 1. Forward pass
    with torch.inference_mode():
        y_logits_test = model_0(X_test).squeeze()
        y_preds_test = torch.round(torch.sigmoid(y_logits_test))

    # 2. Claculate loss 
    test_loss = loss_fn(y_logits_test, y_test)
    test_acc = accuracy_fn(y_true=y_test, y_pred=y_preds_test)
    
    # Print out what's happening
    if epoch % 10 == 0:
        print(f"Epoch: {epoch:3d} | Loss: {loss:.5f} | Acc: {acc:.2f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}")








import requests
from pathlib import Path 

# Download helper functions from Learn PyTorch repo (if not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)

from helper_functions import plot_predictions, plot_decision_boundary


# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test)





class CircleModleV1(nn.Module): 
    def __init__(self): 
        super().__init__()
        self.layer_1 = nn.Linear(in_features=2, out_features=10, bias=True)
        self.layer_2 = nn.Linear(in_features=10, out_features=10, bias=True)
        self.layer_3 = nn.Linear(in_features=10, out_features=1, bias=True)

    def forward(self, x): 
        return self.layer_3(self.layer_2(self.layer_1(x))) 

model_1 = CircleModleV1().to(device)
model_1


# Setup loss function
loss_fn = torch.nn.BCEWithLogitsLoss() 

# Setup optimizer 
optimizer = torch.optim.SGD(params=model_1.parameters(), lr = 0.1)





torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000  # train longer that before

# Put data to target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device) 

for epoch in tqdm(range(epochs)): 
    ### Training 
    model_1.train()

    # 1. Forward pass 
    y_logits = model_1(X_train).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits))  # logits -> pred probs -> pred labels

    # 2 Calculate loss and accuracy 
    loss = loss_fn(y_logits, y_train)
    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)

    # 3. optimizer zero grad 
    optimizer.zero_grad() 

    # 4. Back Prop (loss backward) 
    loss.backward() 

    # 5. Step optimizer (gradient descent) 
    optimizer.step() 

    ### Testing 
    model_1.eval()
    with torch.inference_mode():
        # 1. Forward pass 
        y_logits_test = model_1(X_test).squeeze() 
        y_preds_test = torch.round(torch.sigmoid(y_logits_test))
    
        # 2. Loss and accuracy 
        test_loss = loss_fn(y_logits_test, y_test)
        test_acc = accuracy_fn(y_true=y_test, y_pred=y_preds_test) 

    # print what's happening 
    if epoch%100 == 0: 
        print(f"Epoch: {epoch:4d} | Loss: {loss:.5f} | Acc = {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")

    
        


# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_1, X_test, y_test)





# Create some data (same as notebook 01) 
weight = 0.7 
bias = 0.3 
start = 0 
end = 1 
step = 0.01 

x_regression = torch.arange(start, end, step).unsqueeze(dim=1)
y_regression = weight*x_regression + bias 

print(f'Lenghth of x_regression: {len(x_regression)}') 
x_regression[:5], y_regression[:5]



# Create train and test split 
train_split = int(0.8*len(x_regression)) 
x_regression_train, y_regression_train = x_regression[:train_split], y_regression[:train_split] 
x_regression_test, y_regression_test = x_regression[train_split:], y_regression[train_split:] 

len(x_regression_train), len(y_regression_train), len(x_regression_test), len(y_regression_test)

x_regression_train[:5]


plot_predictions(train_data=x_regression_train, 
                 train_labels=y_regression_train, 
                 test_data=x_regression_test, 
                 test_labels=y_regression_test)





# same architecture as model_1 but using nn.sequential 
model_2 = nn.Sequential(
    nn.Linear(in_features=1, out_features=10),
    nn.Linear(in_features=10, out_features=10), 
    nn.Linear(in_features=10, out_features=1)
).to(device) 

model_2


# setup loss function 
loss_fn = nn.L1Loss() 

# setup optimizer 
optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.01)   


# Train the model 
torch.manual_seed(42) 
torch.cuda.manual_seed(42) 

epochs = 1000 

x_regression_train, y_regression_train = x_regression_train.to(device), y_regression_train.to(device) 
x_regression_test, y_regression_test = x_regression_test.to(device), y_regression_test.to(device) 

for epoch in tqdm(range(epochs)): 
    ### Training 
    model_2.train() 

    # 1. Forward pass 
    y_regression_preds = model_2(x_regression_train) 

    # 2. Calculate loss and accuracy 
    loss = loss_fn(y_regression_preds, y_regression_train) 
    acc = accuracy_fn(y_true=y_regression_train, y_pred=y_regression_preds) 

    optimizer.zero_grad() 

    # 4. loss backward (back prop) 
    loss.backward() 

    # 5. step optimizer (gradient descent) 
    optimizer.step() 

    ### Testing 
    model_2.eval() 
    with torch.inference_mode(): 

        # 1. Forward pass 
        regression_test = model_2(x_regression_test) 

        # 2. Calculate loss and accuracy 
        test_loss = loss_fn(regression_test, y_regression_test) 
        test_acc = accuracy_fn(y_true=y_regression_test, y_pred=regression_test) 

    # print what's happening 
    if epoch%100 == 0: 
        print(f"Epoch: {epoch:4d} | Loss: {loss:.5f} | Acc = {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")






# Turn on evaluation mode
model_2.eval()

# Make predictions (inference)
with torch.inference_mode():
    y_preds = model_2(x_regression_test)

# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)
# (try removing .cpu() from one of the below and see what happens)
plot_predictions(train_data=x_regression_train.cpu(),
                 train_labels=y_regression_train.cpu(),
                 test_data=x_regression_test.cpu(),
                 test_labels=y_regression_test.cpu(),
                 predictions=y_preds.cpu());











# Make and plot data 
import matplotlib.pyplot as plt 
from sklearn.datasets import make_circles

n_samples = 1000 

X, y = make_circles(n_samples, noise=0.03, random_state=42)

plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdBu)


# Convert data to tensors and then to train test splits 
import torch 
from sklearn.model_selection import train_test_split

# Turn data into tensors 
X = torch.from_numpy(X).type(torch.float) 
y= torch.from_numpy(y).type(torch.float) 

# Make train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train[:10], y_train[:10]





# Build the model with non-linear activation function 
from torch import nn 
class CircleModelV2(nn.Module): 
    def __init__(self): 
        super().__init__() 
        self.layer_1 = nn.Linear(in_features=2, out_features=10) 
        self.layer_2 = nn.Linear(in_features= 10, out_features=10)
        self.layer_3 = nn.Linear(in_features=10, out_features=1) 
        self.relu = nn.ReLU()   # ReLU is non-linear activation function 

    def forward(self, x):
        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModelV2().to(device) 

model_3


# Setup loss and optimizer 
loss_fn = nn.BCEWithLogitsLoss() 
optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1) 





torch.manual_seed(42) 
torch.cuda.manual_seed(42) 

epochs = 1000 

X_train, y_train = X_train.to(device), y_train.to(device) 
X_test, y_test = X_test.to(device), y_test.to(device) 

for epoch in tqdm(range(epochs)): 
    ### Training 
    model_3.train() 

    # 1. Forward Pass
    y_logits = model_3(X_train).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits)) 

    # 2. Calculate loss and accuracy 
    loss = loss_fn(y_logits, y_train) 
    acc =  accuracy_fn(y_train, y_preds) 

    # 3. optimizer zero grad 
    optimizer.zero_grad() 

    # 4. loss backward (back prop)
    loss.backward() 

    # 5. optimizer step (gradient descent) 
    optimizer.step() 

    ### Testing 
    model_3.eval()
    with torch.inference_mode(): 
        # 1. Forward pass 
        y_logits_test = model_3(X_test).squeeze() 
        y_preds_test = torch.round(torch.sigmoid(y_logits_test)) 

        # 2. Calculate loss and accuracy 
        test_loss = loss_fn(y_logits_test, y_test) 
        test_acc = accuracy_fn(y_test, y_preds_test) 

    # Print what's happening 
    if epoch%100 == 0: 
        print(f"Epoch: {epoch:4d} | Loss: {loss:.5f} | Acc = {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")





# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity





A = torch.arange(-10, 10, 1, dtype=torch.float32)
A


plt.plot(A)





# create the ReLU function
def relu(x: torch.Tensor) -> torch.Tensor: 
    return torch.maximum(torch.tensor(0), x)


# Plot the ReLU activated toy tesnor
plt.plot(relu(A))


# Let's do the same for sigmoid activation function 
def sigmoid(x): 
    return 1 / (1+torch.exp(x))


plt.plot(sigmoid(A))








# Import dependencies 
import torch 
import matplotlib.pyplot as plt 
from sklearn.datasets import make_blobs 
from sklearn.model_selection import train_test_split 

# Set the hyperparameters for creating data 
NUM_FEATURES = 2 
NUM_CLASSES = 4 
RANDOM_STATE = 42 

# Create multi-class data 
X_blob, y_blob = make_blobs(n_samples=1000, 
                            n_features=NUM_FEATURES, 
                            centers=NUM_CLASSES, 
                            cluster_std=1.5, # give the clusters a little shakeup 
                            random_state=RANDOM_STATE)

# Turn data into tensors 
X_blob = torch.from_numpy(X_blob).type(torch.float) 
y_blob = torch.from_numpy(y_blob).type(torch.long) 

# Split the data into training and test sets 
X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, 
                                                                        y_blob, 
                                                                        test_size=0.2, 
                                                                        random_state=RANDOM_STATE)

# Plot the data 
plt.figure(figsize=(10, 7)) 
plt.scatter(X_blob[:,0], X_blob[:,1], c=y_blob, cmap=plt.cm.RdYlBu)






# Create device agnostic code 
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device 


X_blob_train.shape, y_blob_train.shape, torch.unique(y_blob_train)


# Create a multiclass model 
class BlobModel(nn.Module): 
    def __init__(self, input_features, output_features, hidden_units=8): 
        """Initializes the multi-class classification model 

        Args:  
        input_featues (int): Number of input features to the model 
        output_featues (int): Number of output features (output classes) 
        hidden_units (int): Number of hiddent units between layers, default 8 

        Returns: 

        """
        super().__init__() 
        self.linear_layer_stack = nn.Sequential(
            nn.Linear(in_features=input_features, out_features=hidden_units),
            # nn.ReLU(), ## non linear layers are not used at last as the linear layers are found to be make preds properly
            nn.Linear(in_features=hidden_units, out_features= hidden_units), 
            # nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_features)
        )

    def forward(self, x):
        return self.linear_layer_stack(x)

# Create an instance of the BlobModel and send it to the device 
model_4 = BlobModel(input_features=2, 
                    output_features=4, 
                    hidden_units=8).to(device)

model_4





# Create loss function 
loss_fn = torch.nn.CrossEntropyLoss() 

# Create optimizer 
optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1) 





# Let's get some raw output 
model_4.eval() 
with torch.inference_mode(): 
    y_logits = model_4(X_blob_test.to(device))

y_logits[:10]





# How many elements in a single prediction sample? 
model_4(X_blob_test.to(device))[0].shape, NUM_CLASSES


y_blob_test[:10], y_blob_test[0].shape





# convert logits to prediction probabilities 
y_pred_probs = torch.softmax(y_logits, dim=1)
print(y_logits[:10]) 
print(y_pred_probs[:10])





torch.softmax(y_logits, dim=1)[0].sum()





print(y_pred_probs[0]) 
print(torch.argmax(y_pred_probs[0]))





torch.manual_seed(42)
torch.cuda.manual_seed(42) 

# Put data into the device 
X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device) 
X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device) 

epochs = 100 

for epoch in tqdm(range(epochs)): 
    ### Training 
    model_4.train() 

    # 1. Forward Pass 
    y_logits =  model_4(X_blob_train) 
    y_preds = torch.softmax(y_logits, dim=1).argmax(dim=1) 

    # 2. Calculate loss and aaccuracy 
    loss = loss_fn(y_logits, y_blob_train) 
    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_preds)

    # 3. Optimizer zero grad 
    optimizer.zero_grad() 

    # 4. Loss backward (Back Prop) 
    loss.backward() 

    # 5. optimizer step (gradient descent) 
    optimizer.step() 

    ### Testing 
    model_4.eval() 
    with torch.inference_mode(): 
        # 1. Forward Pass 
        y_logits_test = model_4(X_blob_test) 
        y_preds_test = torch.softmax(y_logits_test, dim=1).argmax(dim=1)

        # 2. Calculate loss and accuracy 
        test_loss = loss_fn(y_logits_test, y_blob_test) 
        test_acc = accuracy_fn(y_true=y_blob_test, y_pred=y_preds_test)

    # print what's happeing 
    if epoch%10 == 0: 
        print(f" Epoch: {epoch:4d} | Train Loss: {loss:.5f} | Train Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}")
    





# Make predictions
model_4.eval()
with torch.inference_mode():
    y_logits = model_4(X_blob_test)

# View the first 10 predictions
y_logits[:10]





# Turn predicted logits into prediction probabilities 
y_pred_probs = torch.softmax(y_logits, dim=1) 

# Turn prediction probabilities into prediction labels 
y_pred_labels = torch.argmax(y_pred_probs, dim=1)

# Compare the first 10 model preds and test labels 
print(f"Predictions: {y_pred_labels[:10]} \nTest Labels:{y_blob_test[:10]}") 
print(f"Test Accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_pred_labels)}%")





plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_4, X_blob_train, y_blob_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_4, X_blob_test, y_blob_test)





try:
    from torchmetrics import Accuracy
except:
    get_ipython().getoutput("pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)")
    from torchmetrics import Accuracy

# Setup metric and make sure it's on the target device
torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)

# Calculate accuracy
torchmetrics_accuracy(y_pred_labels, y_blob_test)


torchmetrics_accuracy.plot()












